# -*- coding: utf-8 -*-

# Copyright (C) 2015 Luis LÃ³pez <luis@cuarentaydos.com>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
# USA.


import argparse
import json
import sys


from arroyo import (
    analyze,
    extensions,
    downloads,
    query,
    schema,
    scraper,
)


class Command(extensions.Command):
    COMMAND_NAME = 'dev'

    def configure_command_parser(self, cmd):
        subcmds = cmd.add_subparsers(dest='devcmd')

        #
        # Fetch command (fetch+parse)
        #
        fetch_cmd = subcmds.add_parser('fetch')
        fetch_cmd.add_argument(
            '--provider',
            help='Force some provider')
        fetch_cmd.add_argument(
            '--uri',
            help='URI to parse')
        fetch_cmd.add_argument(
            '--output',
            type=argparse.FileType('w'),
            default=sys.stdout)

        #
        # Parse command
        #
        parse_cmd = subcmds.add_parser('parse')
        parse_cmd.add_argument(
            '--provider',
            required=True)
        parse_cmd.add_argument(
            '--input',
            type=argparse.FileType('r'),
            default=sys.stdin)
        parse_cmd.add_argument(
            '--output',
            type=argparse.FileType('w'),
            default=sys.stdout)
        parse_cmd.add_argument(
            '--type',
            help='Force type')
        parse_cmd.add_argument(
            '--language',
            help='Force language')

        #
        # Scrape command (fetch+parse)
        #
        scrape_cmd = subcmds.add_parser('scrape')
        scrape_cmd.add_argument(
            '--provider',
            required=True)
        scrape_cmd.add_argument(
            '--uri',
            help='URI to parse'),
        scrape_cmd.add_argument(
            '--iterations',
            default=1,
            type=int)
        scrape_cmd.add_argument(
            '--output',
            type=argparse.FileType('w'),
            default=sys.stdout)
        scrape_cmd.add_argument(
            '--type',
            help='Force type')
        scrape_cmd.add_argument(
            '--language',
            help='Force language')

        #
        # analyze
        #
        analyze_cmd = subcmds.add_parser('analyze')
        analyze_cmd.add_argument(
            '--input',
            type=argparse.FileType('r'),
            default=sys.stdin)
        analyze_cmd.add_argument(
            '--output',
            type=argparse.FileType('w'),
            default=sys.stdout)

        #
        # query
        #
        query_cmd = subcmds.add_parser('query')
        query_cmd.add_argument(
            '--input',
            type=argparse.FileType('r'),
            default=sys.stdin)
        query_cmd.add_argument(
            '--output',
            type=argparse.FileType('w'),
            default=sys.stdout)
        query_cmd.add_argument(
            '--filter',
            dest='queryparams',
            action='append',
            default=[])
        query_cmd.add_argument(
            dest='querystring',
            nargs='?')

        #
        # query2
        #
        query_cmd = subcmds.add_parser('query2')
        query_cmd.add_argument(
            '--output',
            type=argparse.FileType('w'),
            default=sys.stdout)
        query_cmd.add_argument(
            '--filter',
            dest='queryparams',
            action='append',
            default=[])
        query_cmd.add_argument(
            dest='querystring',
            nargs='?')

        #
        # download
        #
        download_cmd = subcmds.add_parser('download')
        download_cmd.add_argument(
            '--input',
            type=argparse.FileType('r'),
            default=sys.stdin)
        download_cmd.add_argument(
            '--add',
            action='store_true'
        )
        download_cmd.add_argument(
            '--list',
            action='store_true'
        )

    def run(self, app, args):
        if args.devcmd == 'fetch':
            self.run_fetch(app, args)

        elif args.devcmd == 'parse':
            self.run_parse(app, args)

        elif args.devcmd == 'scrape':
            self.run_scrape(app, args)

        elif args.devcmd == 'analyze':
            self.run_analyze(app, args)

        elif args.devcmd == 'query':
            self.run_query(app, args)

        elif args.devcmd == 'query2':
            self.run_query2(app, args)

        elif args.devcmd == 'download':
            self.run_download(app, args)

        elif not args.devcmd:
            raise extensions.CommandUsageError()

        else:
            raise NotImplementedError()

    def run_fetch(self, app, args):
        if not args.provider and not args.uri:
            raise extensions.CommandUsageError()

        engine = scraper.Engine(app.srvs)
        ctx = engine.build_context(args.provider, args.uri)
        result = engine.fetch_one(ctx)
        args.output.write(result)

    def run_parse(self, app, args):
        engine = scraper.Engine()
        ctx = engine.build_context(provider=args.provider,
                                   type=args.type,
                                   language=args.language)
        buffer = args.input.read()

        results = list(engine.parse_one(ctx, buffer))
        output = json.dumps([x.dict() for x in results], indent=2)

        args.output.write(output)

    def run_scrape(self, app, args):
        if not args.provider and not args.uri:
            raise extensions.CommandUsageError()

        engine = scraper.Engine(app.srvs)
        ctxs = engine.build_n_contexts(args.iterations,
                                       args.provider,
                                       args.uri,
                                       type=args.type,
                                       language=args.language)
        results = engine.process(*ctxs)

        output = json.dumps([x.dict() for x in results], indent=2)
        args.output.write(output)

    def run_analyze(self, app, args):
        raw = json.loads(args.input.read())
        if isinstance(raw, dict):
            raw = [raw]

        raw = [schema.Source(**x) for x in raw]
        proc = analyze.analyze(*raw, mp=False)

        output = json.dumps([x.dict() for x in proc],
                            indent=2,
                            default=_json_encode_hook)
        args.output.write(output)

    def do_query(self, app, args):
        def _parse_queryparams(pairs):
            for pair in pairs:
                key, value = pair.split('=', 1)
                if not key or not value:
                    raise ValueError(pair)

                yield (key, value)

        if not args.queryparams and not args.querystring:
            errmsg = "filter or querystring are requierd"
            print(errmsg, file=sys.stderr)
            raise extensions.CommandUsageError()

        q = {}
        if args.querystring:
            q.update(query.Query.fromstring(args.querystring))

        if args.queryparams:
            params = dict(_parse_queryparams(args.queryparams))
            q = query.Query(**params)

        engine = query.Engine()
        try:
            ctx = engine.build_filter(q)
        except query.MissingFiltersError as e:
            errmsg = "Unknow filters: %s"
            errmsg = errmsg % ', '.join(e.args[0])
            print(errmsg, file=sys.stderr)
            raise extensions.CommandUsageError()

        data = json.loads(args.input.read())
        data = [schema.Source(**x) for x in data]
        results = engine.apply(ctx, data)
        results = engine.sort(results)

        results = [[entity.dict(), [src.dict() for src in sources]]
                   for (entity, sources) in results]
        output = json.dumps(results, indent=2,
                            default=_json_encode_hook)
        args.output.write(output)

    def do_query2(self, app, args):
        def _parse_queryparams(pairs):
            for pair in pairs:
                key, value = pair.split('=', 1)
                if not key or not value:
                    raise ValueError(pair)

                yield (key, value)

        if not args.queryparams and not args.querystring:
            errmsg = "filter or querystring are requierd"
            print(errmsg, file=sys.stderr)
            raise extensions.CommandUsageError()

        q = {}
        if args.querystring:
            q = query.Query.fromstring(args.querystring)

        if args.queryparams:
            params = dict(_parse_queryparams(args.queryparams))
            q = query.Query(**params)

        # Setup filters before scrape anything
        query_engine = query.Engine()
        try:
            filters = query_engine.build_filter(q)
        except query.MissingFiltersError as e:
            errmsg = "Unknow filters: %s"
            errmsg = errmsg % ', '.join(e.args[0])
            print(errmsg, file=sys.stderr)
            raise extensions.CommandUsageError()

        # Build scrape ctxs and process them
        scrape_engine = scraper.Engine()
        ctxs = scrape_engine.build_contexts_for_query(q)
        sources = scrape_engine.process(*ctxs)
        sources = analyze.analyze(*sources)

        # Pass sources thru filters
        results = query_engine.apply(filters, sources)
        results = query_engine.sort(results)

        # Output
        results = [[entity.dict(), [src.dict() for src in sources]]
                   for (entity, sources) in results]
        output = json.dumps(results, indent=2,
                            default=_json_encode_hook)
        args.output.write(output)

    def run_download(self, app, args):
        dls = downloads.Downloads()
        if args.list:
            print(repr(dls.get_active()))

        elif args.add:
            data = json.loads(args.input.read())
            data = [
                (schema.Entity(**key),
                 [schema.Source(**src) for src in collection])
                for (key, collection) in data]

            for (key, collection) in data:
                try:
                    dls.add(collection[0])
                except extensions.ExtensionError as e:
                    print("Add '%s' failed. Extension error: %r" %
                          (collection[0], e))

        else:
            raise extensions.CommandUsageError()


def _json_encode_hook(value):
    return str(value)
